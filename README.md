# BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
# Introduction
## In this project, we aimed to build a sentiment analysis model using a dataset of movie reviews from IMDB. The primary objective was to classify reviews as either positive or negative based on the sentiment expressed in the text. We began by loading and preparing the dataset, which included cleaning the text data and converting sentiment labels into a numerical format. Following the data preparation, we conducted exploratory data analysis (EDA) to understand the distribution of sentiments and then proceeded to train a machine learning model to predict sentiment based on the review text.
# Summary
# The process began with loading the IMDB dataset and inspecting its structure, which revealed two key columns: review and sentiment. After renaming these columns to text and labels, we converted the sentiment labels from strings to integers for easier processing. We then performed exploratory data analysis to visualize the sentiment distribution, which indicated a balanced dataset.
# Next, we cleaned the text data by removing HTML tags and punctuation, and we transformed the text into a format suitable for machine learning using vectorization. We split the dataset into training and testing sets and trained a logistic regression model on the training data. Finally, we evaluated the model's performance using classification metrics, providing insights into its accuracy and effectiveness in predicting sentiment. This structured approach not only facilitated a clear understanding of the data but also laid the groundwork for further enhancements and refinements in sentiment analysis techniques.
